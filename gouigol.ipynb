{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus.reader import nltk\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(input_folder, output_file):\n",
    "    with open(output_file, 'w') as output:\n",
    "        for filename in os.listdir(input_folder):\n",
    "            with open(input_folder+filename, 'r') as file:\n",
    "                output.write(file.read())\n",
    "\n",
    "id=[]\n",
    "def read_chunks(input_file):\n",
    "    with open(input_file, 'r') as file:\n",
    "        content = file.readlines()\n",
    "       # print(content[:5])\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        oldline=\"*\"*40\n",
    "        b=False\n",
    "        for line in content:\n",
    "            if line.startswith('*' * 40):\n",
    "                if current_chunk:\n",
    "                    chunks.append(' '.join(current_chunk).replace('\\n',' '))\n",
    "                    current_chunk = []\n",
    "                    b=False\n",
    "            else:\n",
    "                if ( line.startswith('Document') and oldline.startswith('*' * 40)):\n",
    "                    b=True\n",
    "                    line=line.split()\n",
    "                    id.append(line[1])\n",
    "                elif(b):\n",
    "                    current_chunk.append(line)\n",
    "            oldline=line\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk).replace('\\n',' '))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "input_folder = \"Collection//\"\n",
    "output_file = 'LISA'\n",
    "# combine_files(input_folder, output_file)\n",
    "\n",
    "chunks=read_chunks(output_file)\n",
    "id=np.array(id)\n",
    "# _,indices=np.unique(id, axis=0, return_index=True)\n",
    "# id=id[np.sort(indices)]\n",
    "# indices=list(set(np.arange(1,6005))-set(id))\n",
    "# chunks=np.delete(np.array(chunks), indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "MotsVides = nltk.corpus.stopwords.words('english')\n",
    "folder_path=\"Collection//\"\n",
    "letter=\"[A-Za-z]\"\n",
    "sigle=\"[A-Z]\\.(?:[A-Z]\\.)+\"\n",
    "slash=f\"{letter}+/{letter}+\"\n",
    "arobas=\"[\\w]*@[\\w]*\"\n",
    "pointsus=\"\\.{3}\"\n",
    "realNumbers=f\"\\d+(?:[\\.,]\\d+)?(?:%|DA|{letter}*)?\"\n",
    "words=\"\\w+\"\n",
    "casSpecial=f\"(?:{letter}|\\d+(?:[\\.,]\\d+))+(?:-(?:{letter}|\\d+(?:[\\.,]\\d+)?)+)+\"\n",
    "casUnionWords=f\"(?:{letter}|\\d+(?:[\\.,]\\d+))+(?:-(?:{letter}|\\d+(?:[\\.,]\\d+)?)+)*\"\n",
    "def textdocs(inverse,token,porter):\n",
    "     liste_dic=[]\n",
    "     textsdoc=[]\n",
    "     for i in range(len(chunks)):\n",
    "          text =chunks[i]\n",
    "          if not token:\n",
    "               text= text.split()  \n",
    "          else:\n",
    "               text=nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*').tokenize(text)\n",
    "     \n",
    "          if porter:\n",
    "               stemmer = nltk.PorterStemmer()\n",
    "          else:\n",
    "               stemmer = nltk.LancasterStemmer()\n",
    "         \n",
    "          text = [word for word in text if word not in string.punctuation ] \n",
    "      \n",
    "          TermesNormalisation= [stemmer.stem(terme) for terme in text if terme.lower() not in MotsVides]\n",
    "          #TermesNormalisation.sort()\n",
    "          liste_dic.append(nltk.FreqDist(TermesNormalisation))\n",
    "     i=0\n",
    "     for dic in liste_dic:\n",
    "         \n",
    "          maxfreq=max(dic.values())\n",
    "          for terme,freq in dic.items():\n",
    "               n=0\n",
    "               for dico in liste_dic:\n",
    "                    n+=(terme in dico)\n",
    "               poid=(freq/maxfreq )* np.log10(len(liste_dic)/n+1)\n",
    "               \n",
    "               if inverse:\n",
    "                    textsdoc.append(f'{terme} {id[i]} {freq} {poid}')\n",
    "               else:\n",
    "                    textsdoc.append(f'{id[i]} {terme} {freq} {poid}')\n",
    "          i+=1\n",
    "          #print(i)\n",
    "     return textsdoc\n",
    "# with open(folder_path+\"..//resultats//DescripteurSplitLancaster.txt\", \"w\") as fichier:\n",
    "#       fichier.write('\\n'.join(textdocs(0,0,0)))\n",
    "# with open(folder_path+\"..//resultats//DescripteurSplitPorter.txt\", \"w\") as fichier:\n",
    "#      fichier.write('\\n'.join(textdocs(0,0,1)))\n",
    "# with open(folder_path+\"..//resultats//DescripteurTokenLancaster.txt\", \"w\") as fichier:\n",
    "#      fichier.write('\\n'.join(textdocs(0,1,0)))\n",
    "# with open(folder_path+\"..//resultats//DescripteurTokenPorter.txt\", \"w\") as fichier:\n",
    "#      fichier.write('\\n'.join(textdocs(0,1,1)))\n",
    "# with open(folder_path+\"..//resultats//InverseSplitLancaster.txt\", \"w\") as fichier:\n",
    "#      fichier.write('\\n'.join(textdocs(1,0,0)))\n",
    "# with open(folder_path+\"..//resultats//InverseSplitPorter.txt\", \"w\") as fichier:\n",
    "#      fichier.write('\\n'.join(textdocs(1,0,1)))\n",
    "# with open(folder_path+\"..//resultats//InverseTokenLancaster.txt\", \"w\") as fichier:\n",
    "#      fichier.write('\\n'.join(textdocs(1,1,0)))\n",
    "# with open(folder_path+\"..//resultats//InverseTokenPorter.txt\", \"w\") as fichier:\n",
    "#      fichier.write('\\n'.join(textdocs(1,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=[]\n",
    "def visualisation(q,i,dpt,token,porter,methode,k,b):\n",
    "     global a\n",
    "     q_alg=q# a revoir \n",
    "     if dpt==\"term per doc\":\n",
    "         \n",
    "          if token==\"Tokenisation\":\n",
    "               if porter==\"Porter Stemmer\":\n",
    "                    fichier=\"resultats//DescripteurTokenPorter.txt\"\n",
    "               else:\n",
    "                    fichier=\"resultats//DescripteurTokenLancaster.txt\"\n",
    "          else:\n",
    "               if porter==\"Porter Stemmer\":\n",
    "                    fichier=\"resultats//DescripteurSplitPorter.txt\"\n",
    "               else:\n",
    "                    fichier=\"resultats//DescripteurSplitLancaster.txt\"\n",
    "     else:   \n",
    "          if token==\"Tokenisation\":\n",
    "               q=nltk.RegexpTokenizer(f'(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*').tokenize(q)\n",
    "          \n",
    "               if porter==\"Porter Stemmer\":\n",
    "                    stemmer=nltk.PorterStemmer()\n",
    "     \n",
    "                    fichier=\"resultats//InverseTokenPorter.txt\"\n",
    "               else:\n",
    "                  \n",
    "                    stemmer=nltk.LancasterStemmer()\n",
    "                    fichier=\"resultats//InverseTokenLancaster.txt\"   \n",
    "          else:\n",
    "               q=q.split()\n",
    "               if porter==\"Porter Stemmer\":\n",
    "                    stemmer=nltk.PorterStemmer()\n",
    "                    fichier=\"resultats//InverseSplitPorter.txt\"\n",
    "               else:\n",
    "                  \n",
    "                    stemmer=nltk.LancasterStemmer()\n",
    "                    fichier=\"resultats//InverseSplitPorter.txt\"     \n",
    "          q = [word for word in q if word not in string.punctuation ] \n",
    "          q=[stemmer.stem(terme) for terme in q if terme.lower() not in MotsVides ] \n",
    "  \n",
    "     if not dpt ==\"term per doc\":  \n",
    "          file = np.genfromtxt(fichier, delimiter=' ', dtype=object, encoding='utf-8',comments=None)\n",
    "          file = np.array([[item.decode('utf-8') if isinstance(item, bytes) else item for item in row] for row in file])    \n",
    "        \n",
    "          if methode==\"Produit Scalaire\": \n",
    "               dict_poids={}\n",
    "        \n",
    "\n",
    "               for f in file:\n",
    "                     if f[0] in q:\n",
    "                         if f[1] in dict_poids:\n",
    "                              dict_poids[f[1]]+=float(f[-1])\n",
    "                         else:\n",
    "                              dict_poids[f[1]]=float(f[-1])\n",
    "              \n",
    "               a=[[str(key),str(round(val,4))] for key,val in dict_poids.items()]\n",
    "               \n",
    "          elif methode==\"Similarité Cosinus\":\n",
    "               dict_poids,dict_den={},{}\n",
    "               for f in file:\n",
    "                    if f[1] in dict_den:\n",
    "                         dict_den[f[1]]+=float(f[-1])**2\n",
    "                    else:\n",
    "                         dict_den[f[1]]=float(f[-1])**2\n",
    "                    if f[0] in q:\n",
    "                         if f[1] in dict_poids:\n",
    "                              dict_poids[f[1]]+=float(f[-1])\n",
    "                         else:\n",
    "                              dict_poids[f[1]]=float(f[-1])\n",
    "               \n",
    "               a=[[str(key),str(round(float(val)/(math.sqrt(len(q)*dict_den[key])),4))] for key,val in dict_poids.items()]\n",
    "\n",
    "          elif methode==\"Indice de Jaccard\":\n",
    "               dict_poids,dict_den={},{}\n",
    "               for f in file:\n",
    "                    if f[1] in dict_den:\n",
    "                         dict_den[f[1]]+=float(f[-1])**2\n",
    "                    else:\n",
    "                         dict_den[f[1]]=float(f[-1])**2\n",
    "                    if f[0] in q:\n",
    "                         if f[1] in dict_poids:\n",
    "                              dict_poids[f[1]]+=float(f[-1])\n",
    "                         else:\n",
    "                              dict_poids[f[1]]=float(f[-1])\n",
    "               \n",
    "               a=[[str(key),str(val/(len(q)+dict_den[key]-val))] for key,val in dict_poids.items()]\n",
    "             \n",
    "          elif methode==\"BM25\":\n",
    "               dictf={}\n",
    "               N=len(set(file[:,1]))\n",
    "               k=float(k)\n",
    "               b=float(b)\n",
    "               nbrdoc={} \n",
    "               dl={} \n",
    "               for f in file:\n",
    "                    if f[0] in q:\n",
    "                         if f[0] in nbrdoc:\n",
    "                              nbrdoc[f[0]]+=1\n",
    "                         else:   \n",
    "                              nbrdoc[f[0]]=1   \n",
    "                    if f[1] in dl:\n",
    "                         dl[f[1]]+=int(f[-2])\n",
    "                    else:\n",
    "                         dl[f[1]]=int(f[-2])\n",
    "                   \n",
    "               avdl=sum(dl.values())/N\n",
    "               for f in file:\n",
    "                    if f[0] in q:\n",
    "                         freq=float(f[2])\n",
    "                         ni=nbrdoc[f[0]]\n",
    "                         if f[1] in dictf:\n",
    "                              dictf[f[1]]+=((freq)/(k*((1-b)+b*(dl[f[1]]/(avdl)))+freq))*np.log10((N-ni+0.5)/(ni+0.5))\n",
    "                         else:\n",
    "                              dictf[f[1]]=((freq)/(k*((1-b)+b*(dl[f[1]]/(avdl)))+freq))*np.log10((N-ni+0.5)/(ni+0.5))\n",
    "\n",
    "               a=[[str(key),str(round(val,4))] for key,val in dictf.items()]\n",
    "               \n",
    "          elif methode==\"Alg\":              \n",
    "\n",
    "\n",
    "               mot_cle = r\"(?:AND|OR)\"\n",
    "               terme = r\"(?!(AND|OR|NOT))((?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*)\"\n",
    "               termeG = r\"(?:NOT )?\" + terme\n",
    "               v = fr\"{termeG}(?: {mot_cle} {termeG})*\"\n",
    "\n",
    "               dico = {}\n",
    "               if re.fullmatch(v, q_alg):\n",
    "                    matches = re.finditer(termeG, q_alg)\n",
    "                    matched_substrings = [match.group() for match in matches if match.group() not in {'ND', 'R'}]\n",
    "                    indice_liste = [i for i, term in enumerate(matched_substrings) if \"NOT\" in term]\n",
    "                    qk = [t for t in q_alg.split() if re.fullmatch(mot_cle, t)]\n",
    "\n",
    "                    listedoc = set(f[1] for f in file)\n",
    "                    file_array = np.array(file)\n",
    "\n",
    "                    for doc in listedoc:\n",
    "                         mot = np.array([not (np.any((file_array[:, 0] == q[i]) & (file_array[:, 1] == doc))) if i in indice_liste else np.any((file_array[:, 0] == q[i]) & (file_array[:, 1] == doc)) for i in range(len(q))])\n",
    "\n",
    "                         if svd(mot, qk):\n",
    "                              dico[doc] = 1\n",
    "\n",
    "                    a = [[str(key), str(val)] for key, val in dico.items()]\n",
    "          elif methode==\"Coef de Dice\":\n",
    "               dict_poids,dict_den={},{}\n",
    "               for f in file:\n",
    "                    if f[1] in dict_den:\n",
    "                         dict_den[f[1]]+=float(f[-1])**2\n",
    "                    else:\n",
    "                         dict_den[f[1]]=float(f[-1])**2\n",
    "                    if f[0] in q:\n",
    "                         if f[1] in dict_poids:\n",
    "                              dict_poids[f[1]]+=float(f[-1])\n",
    "                         else:\n",
    "                              dict_poids[f[1]]=float(f[-1])\n",
    "               \n",
    "               a=[[str(key),str(round(2*val/(len(q)+dict_den[key]),4))] for key,val in dict_poids.items()]\n",
    "               \n",
    "          \n",
    "     else:\n",
    "          resultat=[]\n",
    "          with open(fichier,'r') as filee:\n",
    "                    for line in filee:\n",
    "                         tabLigne=line.split()\n",
    "                         if tabLigne[0]==q:\n",
    "                              resultat.append(tabLigne)   \n",
    "                 \n",
    "                    \n",
    "          return [resultat,None]+[[],[],[],[],[]]\n",
    "     \n",
    "     if not int(i)==0:     \n",
    "          eva=evaluation(i)\n",
    "          curv=p_r_curve_plot(eva[1],eva[2])\n",
    "     else:\n",
    "          eva=([[],[],[],[],[]],0,0)\n",
    "          curv=None\n",
    "    \n",
    "     liste=[a,curv]+eva[0]\n",
    "     return liste\n",
    "\n",
    "def svd(termes,key_liste):\n",
    "     \n",
    "     if len(key_liste)==1:\n",
    "          if key_liste[0]==\"OR\":\n",
    "               return termes[0] or termes[1]\n",
    "          elif key_liste[0]==\"AND\":\n",
    "               return termes[0] and termes[1]\n",
    "     elif len(key_liste)==0:\n",
    "          return termes[0]\n",
    "     else:\n",
    "          avant=[]\n",
    "          apres=[]\n",
    "          reversed_list = key_liste[::-1]\n",
    "          if \"OR\" in key_liste:#si and alors avant apres and sinon avant apres 1st or\n",
    "          \n",
    "                j=len(key_liste)-1-reversed_list.index(\"OR\")#dernier\n",
    "                i=len(key_liste)\n",
    "               \n",
    "                while(i>=0):\n",
    "                    if(i>j):\n",
    "                        apres.append(termes[i]) \n",
    "                    elif(i<=j):\n",
    "                        avant.append(termes[i])\n",
    "                    i-=1\n",
    "          elif \"AND\"in key_liste:\n",
    "                j=len(key_liste)-1-reversed_list.index(\"AND\")#dernier\n",
    "                i=len(key_liste)\n",
    "                while(i>=0):\n",
    "                    if(i>j):\n",
    "                        apres.append(termes[i]) \n",
    "                    elif(i<=j):\n",
    "                        avant.append(termes[i])\n",
    "                    i-=1\n",
    "          \n",
    "          if type(key_liste[:j]) is bool:\n",
    "               liste=[key_liste[:j]]\n",
    "          else:\n",
    "               liste=key_liste[:j]\n",
    "          if type(key_liste[j+1:]) is bool:\n",
    "               liste2=[key_liste[j+1:]]\n",
    "          else:\n",
    "               liste2=key_liste[j+1:]\n",
    "          #print(avant,liste,apres,liste2,[key_liste[j]])\n",
    "     \n",
    "          return svd([svd(avant,liste[::-1]),svd(apres,liste2[::-1])] ,[key_liste[j]])\n",
    " \n",
    "def p_r_curve_plot(p_values,r_values):\n",
    "     plot = plt.figure()\n",
    "     plt.plot(p_values, r_values)\n",
    "     plt.xlabel(f'recall')\n",
    "     plt.ylabel('precision')\n",
    "     plt.title(f'curve recall / precision')\n",
    "     plot.savefig(\"cruve.png\")\n",
    "     plt.close(plot)\n",
    "     plot = [\"cruve.png\"]\n",
    "     return plot\n",
    "def evaluation(ii):\n",
    "     nom_fichier=\"pertinence//LISARJ.NUM\"\n",
    "     chiffres=[]\n",
    "     Judgement=np.empty((0,2),dtype=int)\n",
    "     with open(nom_fichier, 'r') as fichier:\n",
    "          for ligne in fichier:\n",
    "               chiffres.extend(nltk.RegexpTokenizer(\"\\d+\").tokenize(ligne))\n",
    "          i=0\n",
    "          while i < len(chiffres):\n",
    "               x=int(chiffres[i])\n",
    "               i+=2\n",
    "               y=np.array(chiffres[i:i+int(chiffres[i-1])]).astype(int)\n",
    "               for el in y:\n",
    "                    Judgement=np.vstack((Judgement,[x,el]))\n",
    "               i=i+int(chiffres[i-1])\n",
    "\n",
    "     r=np.array(a).astype(float)\n",
    "     \n",
    "     r = r[r[:, 1].argsort()[::-1]]\n",
    " \n",
    "\n",
    "     rep=[l[1] for l in Judgement if l[0]==(int(ii)) ]\n",
    "     \n",
    "     precision=len(set(r[:,0]) & set(rep))/len(r[:,0])\n",
    "\n",
    "     P5=len(set(r[:5,0]) & set(rep))/5\n",
    "     P10=len(set(r[:10,0]) & set(rep))/10\n",
    "\n",
    "     rappel=len(set(r[:,0]) & set(rep))/len(rep)\n",
    "     F_score=(2*precision*rappel)/(rappel+precision)\n",
    "\n",
    "     p_values,r_values=[],[]\n",
    "     pertinent=0\n",
    "     p_values_inter,r_values_inter=[],np.arange(0,1.1,0.1)\n",
    "     for i in range(1,r.shape[0]+1):\n",
    "          if r[i-1,0] in rep :\n",
    "               pertinent+=1\n",
    "          p_values.append(pertinent/i)\n",
    "          r_values.append(pertinent/len(rep))\n",
    "     \n",
    "     p_values=np.array(p_values)\n",
    "     for i in range(len(r_values_inter)):\n",
    "          indices=[j for j in  range(len(r_values)) if  r_values[j]>=r_values_inter[i]]\n",
    "          if len(indices)==0:\n",
    "               p_values_inter.append(0)\n",
    "          else:\n",
    "               p_values_inter.append(np.max(p_values[indices]))\n",
    "     print(p_values_inter)\n",
    "     print(\"kkk\")\n",
    "     return ([str(precision),str(P5),str(P10),str(rappel),str(F_score)],list(r_values_inter),p_values_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.19.1, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "def changing(i):\n",
    "     i=int(i)\n",
    "     queryfile=\"pertinence//LISA.QUE\"\n",
    "     with open(queryfile,'r') as file :\n",
    "          lignes,query=[],[]\n",
    "          oldline=\"#\"\n",
    "          for line in file:\n",
    "               if line.strip().endswith(\"#\"):\n",
    "                    lignes.append(line.replace('\\n',' ')[:-2])\n",
    "                    query.append(' '.join(lignes))\n",
    "                    lignes=[]\n",
    "               else:\n",
    "                    if not oldline.strip().endswith(\"#\"):\n",
    "                         lignes.append(line.replace('\\n',' '))\n",
    "               \n",
    "               oldline=line\n",
    "     queries = np.array(query)\n",
    "     if i>=1 and i<=len(queries):\n",
    "          return queries[i-1]\n",
    "     else:\n",
    "          return ''\n",
    "def update_visibility(selected):\n",
    "          if selected == \"BM25\":\n",
    "               return {bm: gr.Row(visible=True)}\n",
    "          else:\n",
    "               return {bm: gr.Row(visible=False)}\n",
    "def update_visibility2(selected):\n",
    "     if selected == \"term per doc\":\n",
    "          return {rsd: gr.Row(visible=False)}\n",
    "     else:\n",
    "          return {rsd: gr.Row(visible=True)}\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as block:\n",
    "     with gr.Row():\n",
    "          inputs = [\n",
    "          gr.components.Text(label=\"QUERY\",scale=3),gr.components.Number(label=\"DATASET QUERY\") \n",
    "          ]\n",
    "          inputs[1].change(changing,inputs[1],inputs[0])\n",
    "     with gr.Row():\n",
    "          inputs.extend([\n",
    "                 gr.Radio(\n",
    "          [\"doc per term\", \"term per doc\"], label=\"RECHERCHE PAR DOCUMENT OU PAR TERME:\"\n",
    "     ),\n",
    "          gr.Radio(\n",
    "               [\"Tokenisation\", \"Split\"], label=\"TECHNIQUE DE TOKENISATION:\"\n",
    "          ),\n",
    "          gr.Radio(\n",
    "          [\"Lancaster\", \"Porter Stemmer\"], label=\"TECHNIQUE DE NORMALISATION\"\n",
    "     )\n",
    "        ])\n",
    "     with gr.Row(visible=True) as rsd:\n",
    "          inputs.extend([\n",
    "          gr.Radio(\n",
    "          [\"Produit Scalaire\",\"Similarité Cosinus\",\"Indice de Jaccard\",\"BM25\",\"Alg\",\"Coef de Dice\"], label=\"METHODE D''APPARAIMENT \"\n",
    "     )])\n",
    "     inputs[2].change(update_visibility2, inputs=inputs[2], outputs=[rsd])\n",
    "     with gr.Row(visible=False) as bm:  \n",
    "          inputs.extend([\n",
    "     gr.components.Number(label=\"k\"),\n",
    "     gr.components.Number(label=\"B\")])\n",
    "     inputs[5].change(update_visibility, inputs=inputs[5], outputs=[bm])\n",
    "     with gr.Row():\n",
    "          outputs= [gr.Dataframe(row_count = (1, \"dynamic\"), col_count=(4,\"static\"),  headers=[\"terme\",\"document\", \"Frequence\", \"Poids\"],label=\"Output Data\", interactive=1)]\n",
    "     with gr.Column():\n",
    "          btn=gr.Button(\"Lancer\") \n",
    "     with gr.Row():\n",
    "          eval_output=[gr.Textbox(label=\"PRECISION\"), gr.Textbox(label=\"P5\"), gr.Textbox(label=\"P10\"), gr.Textbox(label=\"RECALL\"), gr.Textbox(label=\"F-SCORE\")]\n",
    "     with gr.Column():\n",
    "          graph=[gr.Gallery(label=\"GRAPH\", columns=(1,2),height=500)]\n",
    "          btn.click(fn=visualisation, inputs=inputs, outputs=outputs+graph+eval_output)\n",
    "block.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
